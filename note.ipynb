{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([0, 1, 2, 3])\n",
      "torch.Size([4, 1])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "d_model = 10\n",
    "max_word = 4\n",
    "position = torch.arange(0,max_word)\n",
    "print(position.shape)\n",
    "print(position)\n",
    "position = torch.arange(0,max_word).float().unsqueeze(1)\n",
    "print(position.shape)\n",
    "print(position)\n",
    "exponent = torch.arange(0,d_model,2)/d_model\n",
    "angle = 1/torch.pow(1000,exponent).float()\n",
    "pe = torch.zeros(max_word, d_model).float()\n",
    "pe.require_grad = False\n",
    "\n",
    "pe[:, 0::2] = torch.sin(position/angle)\n",
    "pe[:, 1::2] = torch.cos(position/angle)\n",
    "print(type(pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1.0000,    3.9811,   15.8489,   63.0957,  251.1887, 1000.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(1000,exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "max_len = 4\n",
    "d_model = 10\n",
    "pe = torch.zeros(max_len, d_model).float()\n",
    "pe.require_grad = False\n",
    "\n",
    "position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "print(type(pe))\n",
    "print(pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'exp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\personal_code\\MunLAB\\note.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/personal_code/MunLAB/note.ipynb#ch0000005?line=0'>1</a>\u001b[0m (math\u001b[39m.\u001b[39;49mlog(\u001b[39m10000.0\u001b[39;49m) \u001b[39m/\u001b[39;49m d_model)\u001b[39m.\u001b[39;49mexp()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'exp'"
     ]
    }
   ],
   "source": [
    "(math.log(10000.0) / d_model).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_d,max_len=4000):\n",
    "        super(PositionalEmbed,self).__init__()\n",
    "        angle = self.get_angle(embed_d,max_len)\n",
    "        pe = torch.zeros(max_len, embed_d)\n",
    "        pe.requires_grad = False\n",
    "        pe[:,0::2] = torch.sin(angle)\n",
    "        print(pe[:,1::2].shape)\n",
    "        pe[:,1::2] = torch.cos(angle)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "        \n",
    "\n",
    "    def get_angle(self, embed_d, max_len):\n",
    "        with torch.no_grad():\n",
    "            pos = torch.arange(0,max_len).float().unsqueeze(1)\n",
    "            print(pos.shape)\n",
    "            angle = 1 / (torch.pow(10000.0,torch.arange(0,embed_d,2)/embed_d))\n",
    "\n",
    "        return  pos * angle\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe.shape)\n",
    "        return x + self.pe[:,:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([1, 4, 4])\n",
      "torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]]]).float()\n",
    "print(x.shape)\n",
    "posEmb_l = PositionalEmbed(4,4)\n",
    "out= posEmb_l(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, embed_d,mask=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.mask = mask\n",
    "        self.d_k = embed_d//n_heads\n",
    "        print(self.d_k)\n",
    "        self.W_q = nn.Linear(embed_d,embed_d//n_heads)#nn.Parameter(torch.ones(embed_d,self.d_k)) #nn.Linear(embed_d,embed_d//n_heads)\n",
    "        self.W_k = nn.Linear(embed_d,embed_d//n_heads)#nn.Parameter(torch.ones(embed_d,self.d_k)) #nn.Linear(embed_d,embed_d//n_heads)\n",
    "        self.W_v = nn.Linear(embed_d,embed_d//n_heads)#nn.Parameter(torch.ones(embed_d,self.d_k)) #nn.Linear(embed_d,embed_d//n_heads)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Q = torch.matmul(x,self.W_q)\n",
    "        # K = torch.matmul(x,self.W_k)\n",
    "        # V = torch.matmul(x,self.W_v)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        K_t = K.transpose(2,1)\n",
    "        print(K_t.shape)\n",
    "        \n",
    "        logit = torch.matmul(Q,K_t)/math.sqrt(self.d_k)\n",
    "\n",
    "        #masking 한 영역을 지운다고 생각하면 됨 softmax 결과 0이 되버림\n",
    "        if self.mask is not None:\n",
    "            logit += (self.mask * -1e9)\n",
    "\n",
    "        attn_weight = F.softmax(logit,dim=1)\n",
    "        attn_score = torch.matmul(attn_weight,V)\n",
    "\n",
    "        return attn_score,attn_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 2, 4])\n",
      "tensor([[[-1.4785, -0.3787],\n",
      "         [-0.9148, -0.2621],\n",
      "         [-0.5847, -0.1903],\n",
      "         [-0.3875, -0.1447]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "att_l = AttentionLayer(2,4)\n",
    "out,_ = att_l(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'F:\\\\data\\\\ETDataset\\\\ETT-small'\n",
    "from data.dataLoader import Dataset_ETT_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\personal_code\\MunLAB\\note.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/personal_code/MunLAB/note.ipynb#ch0000012?line=0'>1</a>\u001b[0m etthset \u001b[39m=\u001b[39m Dataset_ETT_hour(root)\n",
      "File \u001b[1;32mf:\\personal_code\\MunLAB\\data\\dataLoader.py:39\u001b[0m, in \u001b[0;36mDataset_ETT_hour.__init__\u001b[1;34m(self, root_path, flag, size, features, data_path, target, scale, inverse, timeenc, freq, cols)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_path \u001b[39m=\u001b[39m root_path\n\u001b[0;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path \u001b[39m=\u001b[39m data_path\n\u001b[1;32m---> 39\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__read_data__()\n",
      "File \u001b[1;32mf:\\personal_code\\MunLAB\\data\\dataLoader.py:58\u001b[0m, in \u001b[0;36mDataset_ETT_hour.__read_data__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     df_data \u001b[39m=\u001b[39m df_raw[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget]]\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale:\n\u001b[1;32m---> 58\u001b[0m     train_data \u001b[39m=\u001b[39m df_data[border1s[\u001b[39m0\u001b[39m]:border2s[\u001b[39m0\u001b[39m]]\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mfit(train_data\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mtransform(df_data\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'df_data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "etthset = Dataset_ETT_hour(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'S' in ['s']:\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mf:\\personal_code\\MunLAB\\note.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/personal_code/MunLAB/note.ipynb#ch0000014?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/personal_code/MunLAB/note.ipynb#ch0000014?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/personal_code/MunLAB/note.ipynb#ch0000014?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('munlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "160fe335b00f2f82a1dba5cbce41b497e11b9258b7a286f83941bedcc7252c46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
